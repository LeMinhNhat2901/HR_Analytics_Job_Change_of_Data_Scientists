# HR Analytics: Job Change of Data Scientists ğŸ“Š

> **Note:** This project is a practical assignment for the course Programming for Data Science (CSC17104). All Machine Learning algorithms and data processing steps are implemented completely from scratch using only NumPy library, WITHOUT using Scikit-learn for core components.

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![NumPy](https://img.shields.io/badge/NumPy-Only-green.svg)](https://numpy.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A comprehensive machine learning project predicting data scientist job changes using pure NumPy implementations of classification algorithms.

---

## ğŸ“š Table of Contents

1. [Introduction ğŸ”](#1-introduction)
2. [Dataset ğŸ“‚](#2-dataset)
3. [Methodology ğŸ› ï¸](#3-methodology)
4. [Installation & Setup âš™ï¸](#4-installation--setup)
5. [Usage â–¶ï¸](#5-usage)
6. [Results ğŸ†](#6-results)
7. [Project Structure ğŸ—‚ï¸](#7-project-structure)
8. [Challenges & Solutions âš”ï¸](#8-challenges--solutions)
9. [Future Improvements ğŸš€](#9-future-improvements)
10. [Contributors ğŸ‘¥](#10-contributors)
11. [License ğŸ“œ](#11-license)

---

## 1. Introduction ğŸ”

### 1.1 Problem Statement ğŸ§­

A company operating in the Big Data and Data Science domain wants to hire data scientists. After recruitment and training, many employees decide to leave for new opportunities. This causes significant waste in time and training costs.

### 1.2 Motivation & Real-world Applications ğŸ’¡

Predicting which candidates are likely to change jobs immediately after training helps the company:

- **Reduce recruitment and training costs**
- **Focus resources on candidates committed to long-term engagement**
- **Understand factors influencing employee turnover decisions**

### 1.3 Specific Objectives ğŸ¯

- Build a complete data processing pipeline and train models to predict the probability of candidates seeking new jobs (Target = 1) or staying (Target = 0)
- **Technical Requirements:** Use only NumPy to build algorithms (Logistic Regression, KNN, Naive Bayes, Neural Network) and data processing techniques (Encoding, Scaling, Imputation)

---

## 2. Dataset ğŸ“‚

### 2.1 Data Source ğŸŒ

Dataset obtained from Kaggle: [HR Analytics: Job Change of Data Scientists](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists)

### 2.2 Data Characteristics ğŸ“ˆ

- **Size:** Approximately 19,000 rows of candidate information
- **Imbalanced Data:** Class 0 (Not changing jobs) significantly outnumbers Class 1 (Changing jobs)

### 2.3 Key Features Description ğŸ“

The dataset includes both Categorical and Numerical features:

| Feature | Type | Description |
|---------|------|-------------|
| `city_development_index` | Numerical | City development index |
| `gender` | Nominal | Gender |
| `relevent_experience` | Ordinal | Relevant experience |
| `education_level` | Ordinal | Education level |
| `major_discipline` | Nominal | Field of study |
| `experience` | Ordinal | Years of experience |
| `company_size` | Ordinal | Company size |
| `target` | Binary | 0 (Not seeking new job), 1 (Seeking new job) |

---

## 3. Methodology ğŸ› ï¸

This project applies standard Data Science workflow but is entirely hand-coded using NumPy.

### 3.1 Data Processing Pipeline (`src/data_processing.py`) ğŸ§°

I built a `DataProcessor` class supporting "Fit & Transform" methodology to prevent Data Leakage:

#### **Missing Value Imputation** ğŸ©º
- **Numerical:** Fill with Median
- **Categorical:** Fill with Mode (most frequent value)

#### **Encoding** ğŸ”¤
- **Ordinal Encoding:** Map ordered variables (e.g., `education_level`) to integers
- **One-Hot Encoding:** Use NumPy Broadcasting technique to create binary matrices for Nominal variables

#### **Scaling** ğŸ“
- **Min-Max Normalization:** Scale data to range [0, 1]

### 3.2 Algorithms Used (`src/models.py`) ğŸ¤–

#### a. Logistic Regression ğŸ“‰

Uses Gradient Descent to optimize Binary Cross-Entropy loss function.

**Sigmoid Activation:**
$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Loss Function (with L1/L2 Regularization):**
$$J(w,b) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)}\log(\hat{y}^{(i)}) + (1-y^{(i)})\log(1-\hat{y}^{(i)})] + \frac{\lambda}{2m}||w||^2$$

**NumPy Implementation Highlights:**
- Manual gradient computation using matrix operations
- Vectorized update rules: `w = w - learning_rate * dw`

#### b. Naive Bayes (Gaussian) ğŸ§®

Calculates probabilities based on Bayes' theorem with independent feature assumption.

**Gaussian Likelihood:**
$$P(x_i|y) = \frac{1}{\sqrt{2\pi\sigma_y^2}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma_y^2}\right)$$

**NumPy Implementation:**
- Calculate mean and variance per class using `np.mean()` and `np.var()`
- Apply log probabilities to prevent numerical underflow

#### c. K-Nearest Neighbors (Vectorized) ğŸ‘¥

Fully vectorized implementation (no loops for distance computation) for speed optimization.

**Euclidean Distance (Matrix Form):**
$$||A - B||^2 = ||A||^2 + ||B||^2 - 2AB^T$$

**NumPy Implementation:**
```python
# Vectorized distance calculation
dists = np.sum(X_train**2, axis=1) + np.sum(X_test**2, axis=1)[:, np.newaxis] - 2 * np.dot(X_test, X_train.T)
```

#### d. Neural Network (MLP) ğŸ§ 

2-layer neural network (Input â†’ Hidden â†’ Output) with manual Backpropagation.

- **Hidden Layer:** ReLU activation
- **Output Layer:** Sigmoid activation

**NumPy Implementation:**
- Forward propagation using `np.dot()` and custom activation functions
- Backward propagation with chain rule derivatives
- Weight updates using gradient descent

---

## 4. Installation & Setup âš™ï¸

### Prerequisites ğŸ§¾
- Python 3.8 or higher
- pip package manager

### Step 1: Clone Repository ğŸ“¥
```bash
git clone https://github.com/LeMinhNhat2901/HR_Analytics_Job_Change_of_Data_Scientists.git
cd HR_Analytics_Job_Change_of_Data_Scientists
```

### Step 2: Create Virtual Environment (Recommended) ğŸ§ª
```bash
# If you want to work with conda environment
conda create -n hr_analysis python=3.10.16 # You can choose other python versions (>=3.8)
conda activate hr_analysis
# Or you can do this
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies ğŸ“¦
```bash
pip install -r requirements.txt
```

**Note:** The main library is NumPy. Other libraries (Matplotlib, Pandas) are only used for visualization or initial CSV loading.

---

## 5. Usage â–¶ï¸

### 5.1 Run Complete Pipeline âš¡

To execute the entire pipeline (Load data â†’ Preprocessing â†’ Train â†’ Evaluate â†’ Visualize):

```bash
python main.py
```

### 5.2 Output ğŸ“

After execution:
- âœ… Models will be trained and results printed to console
- ğŸ“Š Analysis charts saved to `results/figures/`
- ğŸ“„ Final predictions saved to `results/submission_proba.csv`

### 5.3 Jupyter Notebooks ğŸ““

For detailed analysis, explore notebooks in `notebooks/`:
- `01_data_exploration.ipynb` - Exploratory Data Analysis
- `02_preprocessing.ipynb` - Data preprocessing steps
- `03_modeling.ipynb` - Model training and evaluation

---

## 6. Results ğŸ†

### 6.1 Model Performance Comparison ğŸ“Š

After training and evaluation on Test set:

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|-------|----------|-----------|--------|----------|---------|
| Logistic Regression | 0.7653 | 0.6188 | 0.1450 | 0.2349 | 0.7331 | 
| Naive Bayes | 0.7092 | 0.4325 | 0.5452 | 0.4823 | 0.6832 | 
| KNN (k=5) | 0.7332 | 0.4465 | 0.3067 | 0.3636 | 0.6787 | 
| Neural Network | 0.7724 | 0.5889 | 0.2784 | 0.3780 | 0.7397 |

> **Note:** These metrics are just for references, you have to run dataset by yourself and get real-life scores. Please check console logs when running `main.py` for exact values.

### 6.2 Visualizations ğŸ“ˆ

Charts automatically generated during execution:

1. **Correlation Matrix** - Shows feature correlations
2. **Confusion Matrix** - Evaluates best model accuracy
3. **Target Distribution** - Examines data imbalance

![Sample Results](results/figures/confusion_matrix.png)

### 6.3 Key Findings ğŸ”

- **Best Model:** Naive Bayes achieved highest F1-Score (0.4823), and ROC-AUC (0.6832)
- **Imbalanced Data Impact:** Models show higher precision but lower recall due to class imbalance
- **Feature Importance:** `city_development_index`, `experience`, and `education_level` are strong predictors

---

## 7. Project Structure ğŸ—‚ï¸

```
HR_Analytics_Job_Change_of_Data_Scientists/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Original data (aug_train.csv, aug_test.csv)
â”‚   â””â”€â”€ processed/            # Processed data (.npy files)
â”œâ”€â”€ notebooks/                # Jupyter notebooks for analysis and experimentation
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb
â”‚   â””â”€â”€ 03_modeling.ipynb
â”œâ”€â”€ results/                  # Output results
â”‚   â”œâ”€â”€ figures/              # Chart images (.png)
â”‚   â””â”€â”€ submission.csv        # Prediction results file
â”œâ”€â”€ src/                      # Main source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processing.py    # Data processing class (NumPy only)
â”‚   â”œâ”€â”€ models.py             # ML algorithm implementations (NumPy only)
â”‚   â””â”€â”€ visualization.py      # Visualization code
â”œâ”€â”€ main.py                   # Main script to run the program
â”œâ”€â”€ requirements.txt          # Required libraries
â”œâ”€â”€ LICENSE                   # MIT License
â””â”€â”€ README.md                 # Project documentation
```

### File Descriptions ğŸ“„

- **`src/data_processing.py`**: Contains `DataProcessor` class with fit/transform methods for imputation, encoding, and scaling
- **`src/models.py`**: Pure NumPy implementations of LogisticRegression, NaiveBayes, KNN, and NeuralNetwork classes
- **`src/visualization.py`**: Functions for generating correlation matrices, confusion matrices, and other plots
- **`main.py`**: Orchestrates the entire ML pipeline from data loading to model evaluation

---

## 8. Challenges & Solutions âš”ï¸

Working with the constraint "NumPy only" presented several challenges:

### 8.1 KNN Performance Issues ğŸŒ

**Challenge:** Computing distances between each Test point and all Train points using nested for-loops was extremely slow.

**Solution:** Implemented Vectorization and Broadcasting techniques. Instead of direct computation, I applied the identity expansion formula $||A - B||^2$ to convert to matrix multiplication `np.dot`, improving speed by 50-100x.

### 8.2 One-Hot Encoding Implementation ğŸ§©

**Challenge:** Cannot use `pd.get_dummies` or `sklearn.OneHotEncoder`.

**Solution:** Used Broadcasting comparison `(data[:, None] == categories).astype(int)` to create One-Hot matrices efficiently.

### 8.3 Neural Network Backpropagation ğŸ§¯

**Challenge:** Easy to make mistakes when computing matrix derivatives (Matrix Calculus).

**Solution:** Carefully drafted matrix dimensions (shapes) before coding, used `np.dot` carefully to match data dimensions.

### 8.4 Memory Efficiency ğŸ§ 

**Challenge:** Vectorized operations can consume large amounts of memory with big datasets.

**Solution:** Implemented batch processing for KNN distance calculations and neural network training.

---

## 9. Future Improvements ğŸš€

To enhance project results in the future:

### 9.1 Hyperparameter Tuning ğŸ”§
- Implement manual Grid Search using NumPy to find optimal parameters (learning rate, number of layers, k neighbors)

### 9.2 Handle Imbalanced Data âš–ï¸
- Implement SMOTE or Random Undersampling algorithms using NumPy to rebalance dataset (Target=1 class is currently underrepresented)

### 9.3 Feature Engineering ğŸ› ï¸
- Create new features from existing variables to increase accuracy
- Implement polynomial features and interaction terms

### 9.4 Advanced Algorithms ğŸŒ²
- Implement Decision Trees and Random Forest from scratch
- Add ensemble methods (Bagging, Boosting)

### 9.5 Cross-Validation ğŸ”
- Implement K-Fold Cross-Validation using NumPy for more robust model evaluation

---

## 10. Contributors ğŸ‘¥

**Project developed by:**

- **Name:** LÃª Minh Nháº­t
- **Student ID:** 23120067
- **Class:** CQ2023/21
- **School:** HCMUS
- **GitHub:** [@LeMinhNhat2901](https://github.com/LeMinhNhat2901)
- **Email:** nhat29012005@gmail.com

### ğŸ¤ Contributing

Contributions, issues, and feature requests are welcome! Feel free to check [issues page](https://github.com/LeMinhNhat2901/HR_Analytics_Job_Change_of_Data_Scientists/issues).

**Support & Contact Instructor:** **LÃª Nhá»±t Nam** at lnnam@fit.hcmus.edu.vn

**For Course Questions:**
- Use the provided ZALO group
- Office hours as announced

---

## 11. License ğŸ“œ

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.
This project is submitted for academic evaluation as part of the *Programming for data science** course.

- **Course**: Programming for data science (Lab 02)
- **Institution**: Faculty of Information Technology, University of Science (VNU-HCMC)
- **Instructor**: LÃª Nhá»±t Nam
- **Academic Year**: 2025-2026

**Academic Integrity Statement:**
- All code is original work or properly cited
- External references and libraries are documented
- Collaboration limited to course-approved discussion
- No plagiarism or unauthorized code sharing
---

## ğŸ“š References ğŸ”—

- [Kaggle Dataset](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists)
- [NumPy Documentation](https://numpy.org/doc/)
- [Machine Learning from Scratch](https://github.com/eriklindernoren/ML-From-Scratch)

---

**Â© 2025 University of Science (VNU-HCMC)**  
*Developed for Programming for data science*

---

**â­ If you find this project helpful, please consider giving it a star!**
